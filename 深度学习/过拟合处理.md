1. 权值衰减：添加正则项
2. Dropout：在学习的过程中随机删除神经元

机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单 独进行学习，推理时再取多个模型的输出的平均值。可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例，可以取得模型的平均值。可以理解成，Dropout将集成学习的效果（模拟地）通过一个网络实现了。

训练时，以删除比例p随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递。训练时，每传递一次数据，就会随机选择要删除的神经元。 然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出， 要乘上1-p后再传递。

改进：Inverted Dropout

训练时，以删除比例p随机选出隐藏层的神经元，各个神经元的输出要除以1-p，代替测试阶段的操作。