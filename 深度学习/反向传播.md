$$
\text{权重}ω_{ij}^{(l)}
$$
$$
\text{输入}h_{j}^{(l)}
$$
$$
\text{偏置}bias_{i}^{(l)}\text{、}b_{i}^{(l)}
$$
$$
\text{仿射结果}logit_{i}^{(l)}
$$
$$
\text{输出层输出}y
$$
$$
\text{损失函数}cost
$$
l 代表当前神经元所在层数(l > 0)  L是输出层的序号
i 代表当前神经元在当前层中的序号(i > 0)
j 代表该权重连接的上一层神经元在层中的序号(j > 0)

1. 目的是求损失函数cost对神经网络所有权重ω和偏置b的导数；
2. 对每层每个神经元定义δ：cost对神经元logit值的导数：
$$
\delta_{i}^{(l)}=\frac{\partial cost}{\partial logit_{i}^{(l)}}
$$
3. 容易得出以下结果。问题转化为求cost对所有神经元δ的导数；
$$
\frac{\partial cost}{\partial ω_{i j}^{(l)}}= \delta_{i}^{(l)}h_{j}^{(l-1)}
$$
$$
\frac{\partial cost}{\partial bias_{i}^{(l)}}=\delta_{i}^{(l)}
$$
4. 对于输出层，要先求cost对输出结果的导数，然后有：
$$
\delta_{i}^{(L)}=\nabla_{y} \operatorname{cost} \times \sigma^{\prime}\left(logit_{i}^{(L)}\right)
$$
5. 对于隐藏层，logit可以通过多个不同的神经元对cost起作用，所以要先求出前向一层所有神经元的δ：
$$
\delta_{i}^{(l)}=\sum_{j}ω_{j i}^{(l+1)} \delta_{j}^{(l+1)} \sigma^{\prime}\left(logit_{i}^{(l)}\right)
$$

6. 使用矩阵，对一层的神经元一次性计算导数，写作向量时，一个输入数据、输出数据、一个神经元的偏置都是列向量，一层神经元的权重是行数为神经元个数、列数为权重个数的矩阵：
$$
\delta^{(l)}=\frac{\partial cost}{\partial logit^{(l)}}
$$
$$
\frac{\partial \operatorname{cost}}{\partial ω^{(l)}}=\delta^{(l)} \cdot\left(h^{(l-1)}\right)^{T}
$$
$$
\frac{\partial cost}{\partial \operatorname{bias}^{(l)}}=\delta^{(l)}
$$
$$
\delta^{(L)}=\nabla_{y} \operatorname{cost} \odot \sigma^{\prime}\left(logit^{(L)}\right)
$$
$$
\delta^{(l)}=\left(\left(ω^{(l+1)}\right)^{T} \delta^{(l+1)}\right) \odot \sigma^{\prime}\left(logit^{(l)}\right)
$$


