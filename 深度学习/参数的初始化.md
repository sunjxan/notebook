1. 为抑制过拟合、提高泛化能力，采用权值衰减（weight decay）的策略。简单地说，权值衰减就是一种通过减小权重参数的值来抑制过拟合的方法。 如果想减小权重的值，一开始就将初始值设为较小的值才是正途。

2. 将权重初始值设为 0的话，将无法正确进行学习。 
    严格地说，不能将权重初始值设成一样的值。因为如果l层权重都是一样的，那么l+1层神经元的输入值都是一样的。在误差反向传播法中，l+1层所有的权重值都会进行相同的更新。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化” （严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。

3. 各层的激活值的分布都要求有适当的广度。为什么呢？因为通过 在各层间传递多样性的数据，神经网络可以进行高效的学习。反 过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”（没有充分展示出所有神经元输出数据的能力）的问题，导致学习可能无法顺利进行。
4. Xavier Glorot等人的论文中推荐的权重初始值（俗 称“Xavier初始值”）。为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度。推导出的结论是，如果前一层的节点数为n，则初始值使用方差为1/n 的高斯分布。
5. 激活函数使用ReLU时，推荐用的初始值是Kaiming He等人推荐的初始值，也称为“He初始值”。当前一层的节点数为n时，He初始值使用方差为2/n的高斯分布。当使用 Xavier初始值时，（直观上）可以解释为，因为ReLU的负值区域的值 为0，为了使它更有广度，所以需要2倍的系数。

6. 总结一下，当激活函数使用ReLU时，权重初始值使用He初始值，当 激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。 这是目前的最佳实践。
