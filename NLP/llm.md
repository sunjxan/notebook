[[[https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，vocab_size，基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。
](https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/2?fw=pt

第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用*tokenizer*(标记器)，负责：

- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(*token*)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

一旦我们有了标记器，我们就可以直接将我们的句子传递给它，然后我们就会得到一本字典，它可以提供给我们的模型！剩下要做的唯一一件事就是将输入ID列表转换为张量。

https://zhuanlan.zhihu.com/p/630696264

通常情况下，Tokenizer有三种粒度：word/char/subword

- word: 按照词进行分词，如: `Today is sunday`. 则根据空格或标点进行分割`[today, is, sunday, .]`
- subword：按照词的subword进行分词。如：`Today is sunday.` 则会分割成`[to， day，is ， s，un，day， .]`
- character：按照单字符进行分词，就是以char为最小粒度。 如：`Today is sunday.` 则会分割成`[t， o， d，a，y， .... ，s，u，n，d，a，y， .]`

从上到下，词汇表越小，分词越碎，平均使用token数越多，OOV（Out of Vocab）次数越少，序列长度越长，编码效率越低，模型学习难度越大。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，及其大小vocab_size，词汇表大小与训练数据量（n_tokens）不同，分词器基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持，不同分词器的文件不同，使用方式也不同

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4?fw=pt

使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。

每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。

如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。

最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。

https://zhuanlan.zhihu.com/p/635710004

“男儿何不带吴钩，收取关山五十州。”共有16字。几个tokenizer的分词结果如下：

- LLaMA分词为24个token：

```text
 [ '男', '<0xE5>', '<0x84>', '<0xBF>', '何', '不', '<0xE5>', '<0xB8>', '<0xA6>', '<0xE5>', '<0x90>', '<0xB4>', '<0xE9>', '<0x92>', '<0xA9>', '，', '收', '取', '关', '山', '五', '十', '州', '。'] 
```

- Chinese LLaMA分词为14个token：

```text
[ '男', '儿', '何', '不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。']
```

- ChatGLM-6B分词为11个token：

```text
[ '男儿', '何不', '带', '吴', '钩', ',', '收取', '关山', '五十', '州', '。'] 
```

- Bloom分词为13个token：

```text
 ['男', '儿', '何不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。'] 

```

这些标记具有特定的含义，帮助模型识别句子的开始、结束，以及填充等情况。以下是这些特殊标记的含义：

1. **[CLS]**：Class 通常用于分类任务的输入序列中，表示“分类”。在BERT等模型中，[CLS]标记被添加到输入序列的开头，表示输入的开始，并且与任务相关的分类头部会根据[CLS]标记的隐藏状态来做预测。
2. **[MASK]**：Mask 在预训练语言模型中，[MASK]标记通常用于生成任务，模型需要预测被[MASK]标记的位置上应该是哪个词。在BERT中，一部分输入序列中的词被随机替换为[MASK]标记，模型需要预测原始词汇。
3. **[PAD]**：Pad [PAD]标记通常用于填充序列，使所有序列达到相同的长度，便于批处理。在序列中，[PAD]标记可能会被添加到较短的序列末尾，以使其长度与其他序列相同。
4. **[SEP]**：Separator 通常表示句子的分隔，用于将多个句子连接在一起。在Transformer中，[SEP]标记通常用于在句子之间添加分隔，以区分不同句子。
5. **[UNK]**：Unknown 表示未知的单词或标记。当模型遇到不在词汇表中的词汇或标记时，会用[UNK]标记来代替。



Tokenizer的分词和编码方法需要与预训练模型一致，以确保模型的输入能够被正确地处理。我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。



Transformer模型的一般架构

该模型主要由两个块组成：

- **Encoder (左侧)**: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。
- **Decoder (右侧)**: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。

这些部件中的每一个都可以独立使用，具体取决于任务：

- **Encoder-only models**: 适用于需要理解输入的任务，如句子分类和命名实体识别。
- **Decoder-only models**: 适用于生成任务，如文本生成。
- **Encoder-decoder models** 或者 **sequence-to-sequence models**: 适用于需要根据输入进行生成的任务，如翻译或摘要。

## 注意力层

Transformer模型的一个关键特性是*注意力层*。这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。

同样的概念也适用于与自然语言相关的任何任务：一个词本身有一个含义，但这个含义受语境的影响很大，语境可以是研究该词之前或之后的任何其他词（或多个词）。)https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/2?fw=pt

第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用*tokenizer*(标记器)，负责：

- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(*token*)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

一旦我们有了标记器，我们就可以直接将我们的句子传递给它，然后我们就会得到一本字典，它可以提供给我们的模型！剩下要做的唯一一件事就是将输入ID列表转换为张量。

https://zhuanlan.zhihu.com/p/630696264

通常情况下，Tokenizer有三种粒度：word/char/subword

- word: 按照词进行分词，如: `Today is sunday`. 则根据空格或标点进行分割`[today, is, sunday, .]`
- subword：按照词的subword进行分词。如：`Today is sunday.` 则会分割成`[to， day，is ， s，un，day， .]`
- character：按照单字符进行分词，就是以char为最小粒度。 如：`Today is sunday.` 则会分割成`[t， o， d，a，y， .... ，s，u，n，d，a，y， .]`

从上到下，词汇表越小，分词越碎，平均使用token数越多，OOV（Out of Vocab）次数越少，序列长度越长，编码效率越低，模型学习难度越大。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，及其大小vocab_size，词汇表大小与训练数据量（n_tokens）不同，分词器基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持，不同分词器的文件不同，使用方式也不同

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4?fw=pt

使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。

每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。

如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。

最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。

https://zhuanlan.zhihu.com/p/635710004

“男儿何不带吴钩，收取关山五十州。”共有16字。几个tokenizer的分词结果如下：

- LLaMA分词为24个token：

```text
 [ '男', '<0xE5>', '<0x84>', '<0xBF>', '何', '不', '<0xE5>', '<0xB8>', '<0xA6>', '<0xE5>', '<0x90>', '<0xB4>', '<0xE9>', '<0x92>', '<0xA9>', '，', '收', '取', '关', '山', '五', '十', '州', '。'] 
```

- Chinese LLaMA分词为14个token：

```text
[ '男', '儿', '何', '不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。']
```

- ChatGLM-6B分词为11个token：

```text
[ '男儿', '何不', '带', '吴', '钩', ',', '收取', '关山', '五十', '州', '。'] 
```

- Bloom分词为13个token：

```text
 ['男', '儿', '何不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。'] 

```

这些标记具有特定的含义，帮助模型识别句子的开始、结束，以及填充等情况。以下是这些特殊标记的含义：

1. **[CLS]**：Class 通常用于分类任务的输入序列中，表示“分类”。在BERT等模型中，[CLS]标记被添加到输入序列的开头，表示输入的开始，并且与任务相关的分类头部会根据[CLS]标记的隐藏状态来做预测。
2. **[MASK]**：Mask 在预训练语言模型中，[MASK]标记通常用于生成任务，模型需要预测被[MASK]标记的位置上应该是哪个词。在BERT中，一部分输入序列中的词被随机替换为[MASK]标记，模型需要预测原始词汇。
3. **[PAD]**：Pad [PAD]标记通常用于填充序列，使所有序列达到相同的长度，便于批处理。在序列中，[PAD]标记可能会被添加到较短的序列末尾，以使其长度与其他序列相同。
4. **[SEP]**：Separator 通常表示句子的分隔，用于将多个句子连接在一起。在Transformer中，[SEP]标记通常用于在句子之间添加分隔，以区分不同句子。
5. **[UNK]**：Unknown 表示未知的单词或标记。当模型遇到不在词汇表中的词汇或标记时，会用[UNK]标记来代替。



Tokenizer的分词和编码方法需要与预训练模型一致，以确保模型的输入能够被正确地处理。我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。



Transformer模型的一般架构

该模型主要由两个块组成：

- **Encoder (左侧)**: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。
- **Decoder (右侧)**: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。

这些部件中的每一个都可以独立使用，具体取决于任务：

- **Encoder-only models**: 适用于需要理解输入的任务，如句子分类和命名实体识别。
- **Decoder-only models**: 适用于生成任务，如文本生成。
- **Encoder-decoder models** 或者 **sequence-to-sequence models**: 适用于需要根据输入进行生成的任务，如翻译或摘要。

## 注意力层

Transformer模型的一个关键特性是*注意力层*。这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。

同样的概念也适用于与自然语言相关的任何任务：一个词本身有一个含义，但这个含义受语境的影响很大，语境可以是研究该词之前或之后的任何其他词（或多个词）。
](https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/2?fw=pt

第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用*tokenizer*(标记器)，负责：

- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(*token*)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

一旦我们有了标记器，我们就可以直接将我们的句子传递给它，然后我们就会得到一本字典，它可以提供给我们的模型！剩下要做的唯一一件事就是将输入ID列表转换为张量。

https://zhuanlan.zhihu.com/p/630696264

通常情况下，Tokenizer有三种粒度：word/char/subword

- word: 按照词进行分词，如: `Today is sunday`. 则根据空格或标点进行分割`[today, is, sunday, .]`
- subword：按照词的subword进行分词。如：`Today is sunday.` 则会分割成`[to， day，is ， s，un，day， .]`
- character：按照单字符进行分词，就是以char为最小粒度。 如：`Today is sunday.` 则会分割成`[t， o， d，a，y， .... ，s，u，n，d，a，y， .]`

从上到下，词汇表越小，分词越碎，平均使用token数越多，OOV（Out of Vocab）次数越少，序列长度越长，编码效率越低，模型学习难度越大。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，及其大小vocab_size，词汇表大小与训练数据量（n_tokens）不同，分词器基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持，不同分词器的文件不同，使用方式也不同

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4?fw=pt

使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。

每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。

如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。

最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。

https://zhuanlan.zhihu.com/p/635710004

“男儿何不带吴钩，收取关山五十州。”共有16字。几个tokenizer的分词结果如下：

- LLaMA分词为24个token：

```text
 [ '男', '<0xE5>', '<0x84>', '<0xBF>', '何', '不', '<0xE5>', '<0xB8>', '<0xA6>', '<0xE5>', '<0x90>', '<0xB4>', '<0xE9>', '<0x92>', '<0xA9>', '，', '收', '取', '关', '山', '五', '十', '州', '。'] 
```

- Chinese LLaMA分词为14个token：

```text
[ '男', '儿', '何', '不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。']
```

- ChatGLM-6B分词为11个token：

```text
[ '男儿', '何不', '带', '吴', '钩', ',', '收取', '关山', '五十', '州', '。'] 
```

- Bloom分词为13个token：

```text
 ['男', '儿', '何不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。'] 

```

这些标记具有特定的含义，帮助模型识别句子的开始、结束，以及填充等情况。以下是这些特殊标记的含义：

1. **[CLS]**：Class 通常用于分类任务的输入序列中，表示“分类”。在BERT等模型中，[CLS]标记被添加到输入序列的开头，表示输入的开始，并且与任务相关的分类头部会根据[CLS]标记的隐藏状态来做预测。
2. **[MASK]**：Mask 在预训练语言模型中，[MASK]标记通常用于生成任务，模型需要预测被[MASK]标记的位置上应该是哪个词。在BERT中，一部分输入序列中的词被随机替换为[MASK]标记，模型需要预测原始词汇。
3. **[PAD]**：Pad [PAD]标记通常用于填充序列，使所有序列达到相同的长度，便于批处理。在序列中，[PAD]标记可能会被添加到较短的序列末尾，以使其长度与其他序列相同。
4. **[SEP]**：Separator 通常表示句子的分隔，用于将多个句子连接在一起。在Transformer中，[SEP]标记通常用于在句子之间添加分隔，以区分不同句子。
5. **[UNK]**：Unknown 表示未知的单词或标记。当模型遇到不在词汇表中的词汇或标记时，会用[UNK]标记来代替。



Tokenizer的分词和编码方法需要与预训练模型一致，以确保模型的输入能够被正确地处理。我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。



Transformer模型的一般架构

该模型主要由两个块组成：

- **Encoder (左侧)**: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。
- **Decoder (右侧)**: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。

这些部件中的每一个都可以独立使用，具体取决于任务：

- **Encoder-only models**: 适用于需要理解输入的任务，如句子分类和命名实体识别。
- **Decoder-only models**: 适用于生成任务，如文本生成。
- **Encoder-decoder models** 或者 **sequence-to-sequence models**: 适用于需要根据输入进行生成的任务，如翻译或摘要。

## 注意力层

Transformer模型的一个关键特性是*注意力层*。这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。

同样的概念也适用于与自然语言相关的任何任务：一个词本身有一个含义，但这个含义受语境的影响很大，语境可以是研究该词之前或之后的任何其他词（或多个词）。)https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/2?fw=pt

第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用*tokenizer*(标记器)，负责：

- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(*token*)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

一旦我们有了标记器，我们就可以直接将我们的句子传递给它，然后我们就会得到一本字典，它可以提供给我们的模型！剩下要做的唯一一件事就是将输入ID列表转换为张量。

https://zhuanlan.zhihu.com/p/630696264

通常情况下，Tokenizer有三种粒度：word/char/subword

- word: 按照词进行分词，如: `Today is sunday`. 则根据空格或标点进行分割`[today, is, sunday, .]`
- subword：按照词的subword进行分词。如：`Today is sunday.` 则会分割成`[to， day，is ， s，un，day， .]`
- character：按照单字符进行分词，就是以char为最小粒度。 如：`Today is sunday.` 则会分割成`[t， o， d，a，y， .... ，s，u，n，d，a，y， .]`

从上到下，词汇表越小，分词越碎，平均使用token数越多，OOV（Out of Vocab）次数越少，序列长度越长，编码效率越低，模型学习难度越大。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，及其大小vocab_size，词汇表大小与训练数据量（n_tokens）不同，分词器基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持，不同分词器的文件不同，使用方式也不同

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4?fw=pt

使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。

每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。

如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。

最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。

https://zhuanlan.zhihu.com/p/635710004

“男儿何不带吴钩，收取关山五十州。”共有16字。几个tokenizer的分词结果如下：

- LLaMA分词为24个token：

```text
 [ '男', '<0xE5>', '<0x84>', '<0xBF>', '何', '不', '<0xE5>', '<0xB8>', '<0xA6>', '<0xE5>', '<0x90>', '<0xB4>', '<0xE9>', '<0x92>', '<0xA9>', '，', '收', '取', '关', '山', '五', '十', '州', '。'] 
```

- Chinese LLaMA分词为14个token：

```text
[ '男', '儿', '何', '不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。']
```

- ChatGLM-6B分词为11个token：

```text
[ '男儿', '何不', '带', '吴', '钩', ',', '收取', '关山', '五十', '州', '。'] 
```

- Bloom分词为13个token：

```text
 ['男', '儿', '何不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。'] 

```

这些标记具有特定的含义，帮助模型识别句子的开始、结束，以及填充等情况。以下是这些特殊标记的含义：

1. **[CLS]**：Class 通常用于分类任务的输入序列中，表示“分类”。在BERT等模型中，[CLS]标记被添加到输入序列的开头，表示输入的开始，并且与任务相关的分类头部会根据[CLS]标记的隐藏状态来做预测。
2. **[MASK]**：Mask 在预训练语言模型中，[MASK]标记通常用于生成任务，模型需要预测被[MASK]标记的位置上应该是哪个词。在BERT中，一部分输入序列中的词被随机替换为[MASK]标记，模型需要预测原始词汇。
3. **[PAD]**：Pad [PAD]标记通常用于填充序列，使所有序列达到相同的长度，便于批处理。在序列中，[PAD]标记可能会被添加到较短的序列末尾，以使其长度与其他序列相同。
4. **[SEP]**：Separator 通常表示句子的分隔，用于将多个句子连接在一起。在Transformer中，[SEP]标记通常用于在句子之间添加分隔，以区分不同句子。
5. **[UNK]**：Unknown 表示未知的单词或标记。当模型遇到不在词汇表中的词汇或标记时，会用[UNK]标记来代替。



Tokenizer的分词和编码方法需要与预训练模型一致，以确保模型的输入能够被正确地处理。我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。



Transformer模型的一般架构

该模型主要由两个块组成：

- **Encoder (左侧)**: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。
- **Decoder (右侧)**: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。

这些部件中的每一个都可以独立使用，具体取决于任务：

- **Encoder-only models**: 适用于需要理解输入的任务，如句子分类和命名实体识别。
- **Decoder-only models**: 适用于生成任务，如文本生成。
- **Encoder-decoder models** 或者 **sequence-to-sequence models**: 适用于需要根据输入进行生成的任务，如翻译或摘要。

## 注意力层

Transformer模型的一个关键特性是*注意力层*。这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。

同样的概念也适用于与自然语言相关的任何任务：一个词本身有一个含义，但这个含义受语境的影响很大，语境可以是研究该词之前或之后的任何其他词（或多个词）。
](https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/2?fw=pt

第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用*tokenizer*(标记器)，负责：

- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(*token*)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

一旦我们有了标记器，我们就可以直接将我们的句子传递给它，然后我们就会得到一本字典，它可以提供给我们的模型！剩下要做的唯一一件事就是将输入ID列表转换为张量。

https://zhuanlan.zhihu.com/p/630696264

通常情况下，Tokenizer有三种粒度：word/char/subword

- word: 按照词进行分词，如: `Today is sunday`. 则根据空格或标点进行分割`[today, is, sunday, .]`
- subword：按照词的subword进行分词。如：`Today is sunday.` 则会分割成`[to， day，is ， s，un，day， .]`
- character：按照单字符进行分词，就是以char为最小粒度。 如：`Today is sunday.` 则会分割成`[t， o， d，a，y， .... ，s，u，n，d，a，y， .]`

从上到下，词汇表越小，分词越碎，平均使用token数越多，OOV（Out of Vocab）次数越少，序列长度越长，编码效率越低，模型学习难度越大。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，及其大小vocab_size，词汇表大小与训练数据量（n_tokens）不同，分词器基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持，不同分词器的文件不同，使用方式也不同

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4?fw=pt

使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。

每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。

如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。

最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。

https://zhuanlan.zhihu.com/p/635710004

“男儿何不带吴钩，收取关山五十州。”共有16字。几个tokenizer的分词结果如下：

- LLaMA分词为24个token：

```text
 [ '男', '<0xE5>', '<0x84>', '<0xBF>', '何', '不', '<0xE5>', '<0xB8>', '<0xA6>', '<0xE5>', '<0x90>', '<0xB4>', '<0xE9>', '<0x92>', '<0xA9>', '，', '收', '取', '关', '山', '五', '十', '州', '。'] 
```

- Chinese LLaMA分词为14个token：

```text
[ '男', '儿', '何', '不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。']
```

- ChatGLM-6B分词为11个token：

```text
[ '男儿', '何不', '带', '吴', '钩', ',', '收取', '关山', '五十', '州', '。'] 
```

- Bloom分词为13个token：

```text
 ['男', '儿', '何不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。'] 

```

这些标记具有特定的含义，帮助模型识别句子的开始、结束，以及填充等情况。以下是这些特殊标记的含义：

1. **[CLS]**：Class 通常用于分类任务的输入序列中，表示“分类”。在BERT等模型中，[CLS]标记被添加到输入序列的开头，表示输入的开始，并且与任务相关的分类头部会根据[CLS]标记的隐藏状态来做预测。
2. **[MASK]**：Mask 在预训练语言模型中，[MASK]标记通常用于生成任务，模型需要预测被[MASK]标记的位置上应该是哪个词。在BERT中，一部分输入序列中的词被随机替换为[MASK]标记，模型需要预测原始词汇。
3. **[PAD]**：Pad [PAD]标记通常用于填充序列，使所有序列达到相同的长度，便于批处理。在序列中，[PAD]标记可能会被添加到较短的序列末尾，以使其长度与其他序列相同。
4. **[SEP]**：Separator 通常表示句子的分隔，用于将多个句子连接在一起。在Transformer中，[SEP]标记通常用于在句子之间添加分隔，以区分不同句子。
5. **[UNK]**：Unknown 表示未知的单词或标记。当模型遇到不在词汇表中的词汇或标记时，会用[UNK]标记来代替。



Tokenizer的分词和编码方法需要与预训练模型一致，以确保模型的输入能够被正确地处理。我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。



Transformer模型的一般架构

该模型主要由两个块组成：

- **Encoder (左侧)**: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。
- **Decoder (右侧)**: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。

这些部件中的每一个都可以独立使用，具体取决于任务：

- **Encoder-only models**: 适用于需要理解输入的任务，如句子分类和命名实体识别。
- **Decoder-only models**: 适用于生成任务，如文本生成。
- **Encoder-decoder models** 或者 **sequence-to-sequence models**: 适用于需要根据输入进行生成的任务，如翻译或摘要。

## 注意力层

Transformer模型的一个关键特性是*注意力层*。这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。

同样的概念也适用于与自然语言相关的任何任务：一个词本身有一个含义，但这个含义受语境的影响很大，语境可以是研究该词之前或之后的任何其他词（或多个词）。)https://juejin.cn/post/7028579355643265038
计算机无论如何都无法理解人类语言，它只会计算，不过就是通过计算，它让你感觉它理解了人类语言。
它面临文字的时候，都是要通过数字去理解的。所以，如何把文本转成数字，这是NLP中最基础的一步。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/2?fw=pt

第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用*tokenizer*(标记器)，负责：

- 将输入拆分为单词、子单词或符号（如标点符号），称为标记(*token*)
- 将每个标记(token)映射到一个整数
- 添加可能对模型有用的其他输入

一旦我们有了标记器，我们就可以直接将我们的句子传递给它，然后我们就会得到一本字典，它可以提供给我们的模型！剩下要做的唯一一件事就是将输入ID列表转换为张量。

https://zhuanlan.zhihu.com/p/630696264

通常情况下，Tokenizer有三种粒度：word/char/subword

- word: 按照词进行分词，如: `Today is sunday`. 则根据空格或标点进行分割`[today, is, sunday, .]`
- subword：按照词的subword进行分词。如：`Today is sunday.` 则会分割成`[to， day，is ， s，un，day， .]`
- character：按照单字符进行分词，就是以char为最小粒度。 如：`Today is sunday.` 则会分割成`[t， o， d，a，y， .... ，s，u，n，d，a，y， .]`

从上到下，词汇表越小，分词越碎，平均使用token数越多，OOV（Out of Vocab）次数越少，序列长度越长，编码效率越低，模型学习难度越大。

http://www.cnblogs.com/End1ess/p/16165854.html
分词器Tokenizer独立于模型存在，在录入语料库后，有了词汇表，及其大小vocab_size，词汇表大小与训练数据量（n_tokens）不同，分词器基本工作：分词、标记化
加载分词器要有vocab文件和配置json文件支持，不同分词器的文件不同，使用方式也不同

tokenizer.tokenize 将字符串转换为token序列

tokenizer.convert_tokens_to_string ['长', '安'] -> '长安'
tokenizer.convert_ids_to_tokens [7270， 2128] -> ['长', '安']
tokenizer.convert_tokens_to_ids ['长', '安'] -> [7270， 2128]

tokenizer.encode 将字符串转换为id序列 tokenize + convert_tokens_to_ids
tokenizer.decode 将id序列转换为字符串 convert_ids_to_tokens + convert_tokens_to_string

以max_seq_length作标准，将序列填充或截断，填充使用的token是[PAD]，从左或从右。
统一长度的序列，就是NLP要的素材。

https://huggingface.co/learn/nlp-course/zh-CN/chapter2/4?fw=pt

使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。

每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。

如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。

最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。

https://zhuanlan.zhihu.com/p/635710004

“男儿何不带吴钩，收取关山五十州。”共有16字。几个tokenizer的分词结果如下：

- LLaMA分词为24个token：

```text
 [ '男', '<0xE5>', '<0x84>', '<0xBF>', '何', '不', '<0xE5>', '<0xB8>', '<0xA6>', '<0xE5>', '<0x90>', '<0xB4>', '<0xE9>', '<0x92>', '<0xA9>', '，', '收', '取', '关', '山', '五', '十', '州', '。'] 
```

- Chinese LLaMA分词为14个token：

```text
[ '男', '儿', '何', '不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。']
```

- ChatGLM-6B分词为11个token：

```text
[ '男儿', '何不', '带', '吴', '钩', ',', '收取', '关山', '五十', '州', '。'] 
```

- Bloom分词为13个token：

```text
 ['男', '儿', '何不', '带', '吴', '钩', '，', '收取', '关', '山', '五十', '州', '。'] 

```

这些标记具有特定的含义，帮助模型识别句子的开始、结束，以及填充等情况。以下是这些特殊标记的含义：

1. **[CLS]**：Class 通常用于分类任务的输入序列中，表示“分类”。在BERT等模型中，[CLS]标记被添加到输入序列的开头，表示输入的开始，并且与任务相关的分类头部会根据[CLS]标记的隐藏状态来做预测。
2. **[MASK]**：Mask 在预训练语言模型中，[MASK]标记通常用于生成任务，模型需要预测被[MASK]标记的位置上应该是哪个词。在BERT中，一部分输入序列中的词被随机替换为[MASK]标记，模型需要预测原始词汇。
3. **[PAD]**：Pad [PAD]标记通常用于填充序列，使所有序列达到相同的长度，便于批处理。在序列中，[PAD]标记可能会被添加到较短的序列末尾，以使其长度与其他序列相同。
4. **[SEP]**：Separator 通常表示句子的分隔，用于将多个句子连接在一起。在Transformer中，[SEP]标记通常用于在句子之间添加分隔，以区分不同句子。
5. **[UNK]**：Unknown 表示未知的单词或标记。当模型遇到不在词汇表中的词汇或标记时，会用[UNK]标记来代替。



Tokenizer的分词和编码方法需要与预训练模型一致，以确保模型的输入能够被正确地处理。我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。



Transformer模型的一般架构

该模型主要由两个块组成：

- **Encoder (左侧)**: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。
- **Decoder (右侧)**: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。

这些部件中的每一个都可以独立使用，具体取决于任务：

- **Encoder-only models**: 适用于需要理解输入的任务，如句子分类和命名实体识别。
- **Decoder-only models**: 适用于生成任务，如文本生成。
- **Encoder-decoder models** 或者 **sequence-to-sequence models**: 适用于需要根据输入进行生成的任务，如翻译或摘要。

## 注意力层

Transformer模型的一个关键特性是*注意力层*。这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。

同样的概念也适用于与自然语言相关的任何任务：一个词本身有一个含义，但这个含义受语境的影响很大，语境可以是研究该词之前或之后的任何其他词（或多个词）。
