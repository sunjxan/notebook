## 分词 （词典分词、机器学习分词）
### 词典分词
最长匹配算法：双指针搜索（正向、逆向、综合）

速度优化
1. 查询词典优化
   有序集合 => 字典树 => 首字散列字典树 => 双数组字典树（全散列，快速确定下个字符是否匹配，不适合快速获取所有下个字符）
2. 剪枝优化
   利用字典树的前缀树属性剪枝：提前移动左指针（正向匹配）
3. 预处理优化
   先用AC自动机获得完全切分的结果，计算每个位置的最长匹配词
4. 结合
   使用双数组字典数构造的AC自动机进行预处理，记住每个位置的最长匹配词，再进行最长匹配算法

准确率

精确率P=分词结果中开始和结束位置都正确的数量 / 分词结果的词数

召回率R=分词结果中开始和结束位置都正确的数量 / 正确结果的词数

根据正确结果的词是否出现在词典或训练集中，召回率R可以分为R(IV)、R(OOV)。R(IV)低是欠拟合，R(IV)高R(OOV)低是过拟合。

F1=P和R的调和平均数=2/(1/P + 1/R)

### 机器学习分词

序列标注 Begin Middle End Single

状态序列y:标注序列（隐状态）、观测序列x:字符序列（显状态）

机器学习要解决的根本问题就是给定随机变量x，预测随机变量y的问题。分为回归和分类两大类。
根据y是一个独立变量还是多个相互关联的变量，分类问题又分为
1.分类问题 2.结构化预测问题(通常是由整个序列的每个元素的分类问题组成，要考虑元素的分类和结构整体的合理程度)
这也是自燃语言处理关注的两类问题。

根据建模的目标分为
1.生成式模型 模型参数可以生成状态发生的概率p(y)和状态产生观测结果的概率p(x|y)，都有隐马尔可夫模型的三大问题
2.判别式模型 只针对观测结果x到隐藏状态y的条件概率p(y|x)的建模，x内部复杂的依赖关系不影响对y的判断

根据学习方式分为 1.计数型 2.迭代型

概率图模型PGM 有向图模型DGM 无向图模型UGM
有向图模型将概率分解为一系列v的所有前驱结点到结点v的条件概率之积，经常用生成式模型实现。
无向图模型不探究每个事件的因果关系，不涉及条件概率分解。无向图模型将概率分解为所有最大团上的因子函数之积p/归一化因子Z，Z即是所有可能下p的和。最大团是满足所有结点都相互连接的最大子图。添加虚拟因子结点，拆分为更小的最大团。经常用判别式模型实现，计算p(y|x)时的归一化因子Z即是确定的x下所有可能y的p之和。

#### 统计语法模型
语言模型LM就是给定一个句子w，计算句子出现的概率P(w)的模型。
根据训练语料库中分词的结果，枚举出所有可能的分词结果，并作出有向无环图，根据 统计语法模型设置每条边的花费，起点到终点所有路径中花费和最少的为分词结果。
贪心算法可能得到错误结果，暴力计算太耗时，可以使用动态规划算法，每个结点维护从起点到该结点的最小花费以及路径，遍历上一层每个结点的结果来更新自己。

#### 隐马尔可夫模型HMM
初始状态概率向量、状态转移概率矩阵、发射概率矩阵
三个问题：状态序列预测、样本生成及计算出现概率、训练模型参数
前两个问题使用动态规划算法，最优子结构都是某个结点是某种状态同时产生正确观测结果的概率与轨迹
训练模型使用计数的频率数据预估模型参数

#### 结构化感知机
1.对于一个样本句子和标注，给出一个打分。对一个标注结果(标注, 字)提取一些特征，对两个连续的标注(标注1, 标注2)提取一些特征，每种特征取值有0、1两种，都有权重系数。遍历整个句子，计算所有迭代中特征的加权和，作为打分结果；
2.使用动态规划算法，计算根据当前权重能得到最高分数的标注序列；
3.比较最高分数的标注序列与正确结果，标注正确的位置，增加特征值为1的特征权重；标注错误的位置，减小特征值为1的权重。

#### 条件随机场CRF
传统手法中最准确的中文分词器
条件随机场的概率计算即是将结构化感知机的打分结果通过softmax函数转换得到，预测算法也相同，只有训练算法不同。
