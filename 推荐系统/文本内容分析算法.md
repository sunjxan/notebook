[原网页](https://lumingdong.cn/text-content-analysis-algorithm.html)
## *1.*TF-IDF

在信息检索（Information Retrieval）、文本挖掘（Text Mining）以及自然语言处理（Natural Language Processing）领域，**TF-IDF（term frequency–inverse document frequency，词频-逆文档频率，简称 tf-idf）**算法都可以说是鼎鼎有名。虽然在这些领域中，目前也出现了不少以深度学习为基础的新的文本表达和算分（Weighting）方法，但是 TF-IDF 作为一个最基础的方法，依然在很多应用中发挥着不可替代的作用。

TF-IDF 算法在基于内容的推荐系统中，发挥的作用主要有两个，一个是关键词提取，来获取最基础的标签；另外一个作用就是为所有属性标签赋值（Weighting）。

**文本向量化的过程**有点类似 One-Hot，以新闻资讯类推荐为例，记我们要表示的所有文章集合为 D={d1,d2,⋯,dn}，而所有文章中出现的词（对于中文文章，首先得对所有文章进行分词）的集合（也称为词典）为 T={t1,t2,⋯,tn}。也就是说，我们有 N 篇要处理的文章，而这些文章里包含了 n 个不同的词。我们最终要使用一个向量来表示一篇文章，比如第 j 篇文章被表示为 dj={w1j,w2j,⋯,wnj}，其中 wij 表示第 i 个词 ti 在文章 j 中的权重，值越大表示越重要，dj 中其他向量的解释类似。所以，为了表示第 j 篇文章，现在关键的就是如何计算 dj 各分量的值了。例如有一种最简单的方法，如果词 ti 出现在第 j 篇文章中，对应的词权重 wij 赋值为 1，没有出现的词都赋值为 0，但是这种方法无法表达关键词在文档中的重要程度，此时你可能会想到赋值 wij 为词 ti 出现在第 j 篇文章中的次数（frequency），这种方法也有缺点，因为语言的因素，有一些词可能会比较自然地在很多文档中反复出现，比如英语中的 “The”、“An”、“But” 等等。这些词大多起到了链接语句的作用，是保持语言连贯不可或缺的部分，你不能说这些词数量多就说明它重要。因此 TF-IDF 便应运而生，它能够很好的表达词在文档中的重要程度。由此可知，**TF-IDF 就是在向量空间模型的假设下的一种更加复杂的赋值方式。**

TF-IDF 如何计算呢， 最基础的模式，顾名思义，就是 TF 和 IDF 的乘积。

我们先来看整个公式：



TF−IDFScore(ti,dj)=TF(ti,dj)×IDF(ti)=TF(ti,dj)×log(Nni+1)



其中，TF 表示词频（Term Frequency），IDF 表示逆文档频率（Inverse Document Frequency），TF(ti,dj) 表示第 i 个词 ti 在第 j 篇文档 dj 中出现的次数，N 为所有文档总数，ni 表示有 n 篇文档包含第 i 个词，也就是 DF（Document Frequency）文档频率。

我们根据 TF-IDF 算法演变[1](https://lumingdong.cn/text-content-analysis-algorithm.html#dfref-footnote-1)来理解这个公式的具体含义。

1）最初的想法是利用文章的词频来表示词在文章中的权重，但是有很多像连接词这样的高频词会使得仅使用 TF 无法帮助我们区分文档的相关度，因次我们需要去 “惩罚”（Penalize）那些出现在太多文档中的词。IDF 就在这样的情况下应运而生，即用 DF 的倒数来修正 TF，DF 值越大该词越不重要。其基本思想是认为真正携带 “相关” 信息的词仅仅出现在相对比较少，有时候可能是极少数的文档里。



W1=TF×1DF



2）经过一段时间，人们发现 TF 的值在原始的定义中没有任何上限。虽然我们一般认为一个文档包含查询关键词多次相对来说表达了某种相关度，但这样的关系很难说是线性的。比如 A 文档出现 “车” 这个词 100 次，而 B 文档出现 “车” 这个词可能有 200 次，是不是说文档 B 的相关度就是文档 A 的 2 倍呢？其实，很多人意识到，超过了某个阈值之后，这个 TF 也就没那么有区分度了。因此，人们开始使用对称函数对 TF 进行处理（多用 1+log(TF)，稍作平滑），让原来 TF 的线性增长变为对数增长，这样的计算保持了一个平衡，既有区分度，但也不至于完全线性增长



W2=[1+log(TF)]×1DF



3）再然后，人们发现经典的计算并没有考虑 “长文档” 和 “短文档” 的区别。一个文档 A 有 3,000 个词，一个文档 B 有 250 个词，很明显，即便 “车” 在这两个文档中都同样出现过 20 次，也不能说这两个文档都同等相关。因此，需要对 TF 进行 “标准化”（Normalization），常用的是根据文档的最大 TF 值进行的标准化（有时会用双 0.5 处理进行平滑）。



W3=0.5+0.5×TFTFmaxj×1DF



4）还有个技巧就是对 IDF 进行处理，也是我们最常用的一个 TF-IDF 的变种，它可以解决上面提到的几个问题。相对于直接使用 IDF 来作为 “惩罚因素”，我们可以使用文档总数 N 除以 DF + 1（加 1 平滑，可避免分母为 0）作为一个新的 DF 的倒数，并且再在这个基础上通过一个对数变化。这样做的好处就是，第一，使用了文档总数来做标准化，很类似上面提到的标准化的思路，所有词的 N 都是一样的，因此出现文本数越少 (DF or n) 的词，它的 IDF 值越大；第二，利用对数来达到非线性增长的目的。



W4=TF×log(NDF+1)



期间也有其他变种，有修正 TF 的，也有修正 IDF 的，不过要解决的问题和思路基本一致，可参考下图[2](https://lumingdong.cn/text-content-analysis-algorithm.html#dfref-footnote-2)：

[![img](https://lumingdong.cn/wp-content/uploads/2019/02/figure-1-1024x335.png)](https://lumingdong.cn/wp-content/uploads/2019/02/figure-1.png)

[![img](https://lumingdong.cn/wp-content/uploads/2019/02/tfidf-example-use-1024x398.png)](https://lumingdong.cn/wp-content/uploads/2019/02/tfidf-example-use.png)

TF-IDF 在实际使用时还应注意：

1）我们发现常用的标准公式中，TF 是没有进行标准化的，对于长短文本混合文档的词权重计算，我们可以像上述第三种变种那样，对 TF 进行标准化，除以该文档的最大 TF 值（即出现次数最多的那个词的词频，有时候也会除以该文档总词数），然后再乘以 IDF 值。



TF−IDF=TFTFmaxj×log(Nn+1)



2）另外一种方式是做全局归一化，先按标准公式计算得到每一个词的 TF-IDF 值，然后针对该文档的所有词的 TF 总值做归一化，这样不同文字描述的表示向量被归一到一个量级上，具体公式如下：



wi,j=TF−IDF(ti,dj)∑ts∈dj[TF−IDF(ts,dj)]2−−−−−−−−−−−−−−−−−−−√



3）对于英文文本，可直接取单词；对于中文文章，需要先进行分词， 常用的开源分词工具有结巴分词、中科院分词等，这一条对于很多中文本文分析都适用。

有了词权重，根据该权重筛选关键词的方式有：

1. 给定一个 K，取 Top K 个词，这样做简单直接，但也有一点，如果总共得到的词个数少于 K，那么所有词都是关键词了，显然这样做不合理；
2. 计算所有词权重的平均值，取在权重在平均值之上的词作为关键词；

另外，在某些场景下，还会加入以下其他的过滤措施，如：只提取动词和名词作为关键词[3](https://lumingdong.cn/text-content-analysis-algorithm.html#dfref-footnote-3)。

## *2.*TextRank

TextRank 与 TD-IDF 类似，也是常被用来做关键词提取的算法，当然也可被用于向量化的赋值算法。不过 TF-IDF 仅仅从词的统计信息出发，而没有充分考虑词之间的语义信息，而 TextRank 则是一种考虑了相邻词的语义关系、基于图排序的关键词提取算法。

TextRank 源自著名的 PageRank 算法，是 Mihalcea 与 Tarau 于 EM[NLP](https://lumingdong.cn/tag/nlp)'04 提出来的。其思想非常简单：通过词之间的相邻关系构建网络，然后用 PageRank 迭代计算每个结点的 rank 值，排序 rank 值即可得到关键词。

先来看看 PageRank 算法，PageRank 设计之初是用于 Google 的网页排名的，用它来体现网页的相关性和重要性，也就是搜索引擎优化（SEO）中经常被提到的 PR 值。

PageRank 算法的思想很简单，把每一个网页都看做图中的结点，网页间的连接关系即为图的边，因此，每个网页都有一个输出链接（Outlink，简称出链）集合以及一个输入链接（Outlink，简称入链）集合，同时每个网页本身设定一个 PR 值，代表这个网页重要度或权威性，这个 PR 值也是我们要计算的，它是这么定义的，当前页面的 PR 值，是该网页的所有输入链接 PR 值的加权和。具体而言，我们可以把网页中的链接看作是一种投票，如果要计算网页 A 的 PR 值，就需要知道其他网页给网页 A 投了多少票，也就是首先要得到网页 A 的入链。不过每一条入链所投票的多少还要看投票主人身份的，来源网页的 PR 值越高，其权威性越高，网页 A 可能获得的投票数就越多。另外还需要考虑的一点，就是来源网页到底给多少网页投了票，投出的票太多会稀释它本身的权威性，所以入链投票数一般等于来源网页 PR 值对出链（所有投票数）的平均值。那么根据以上思想并加以优化，便有了 PageRank 算法的公式：



PR(Vi)=(1−d)+d∗∑j∈In(Vi)1|Out(Vj)|PR(Vj)



其中，d 表示阻尼系数 (damping factor)，用于平滑，通常取值为 0.60~0.85；Vi 表示某个网页，Vj 表示链接到 Vi 的网页（即 Vi 的入链）；PR(Vi) 表示网页 Vi 的 PR 值，In(Vi) 表示网页 Vi 的所有入链的集合；|Out(Vj)| 表示来源网页的出链总数，1|Out(Vj)| 表示所有出链指向的网页应该平分 Vj 的 PR 值，这样才算是把自己的票分给了自己链接到的网页。

需要注意的是，该公式是一个迭代公式，因为在最初的时候，所有网页都没有算好的 PR 值，因此会给所有网页一个相同的初始值，比如 1，然后按照上面公式迭代计算，直至收敛。

加入阻尼系数 d 那几个部分是为了克服仅用求和部分的一些固有缺陷：如果仅仅有求和的部分，那么该公式将无法处理没有入链的网页的 PR 值，因为这时，根据该公式这些网页的 PR 值为 0，会影响收敛。所以加入了一个阻尼系数来确保每个网页都有一个大于 0 的 PR 值，根据实验的结果，在 0.85 的阻尼系数下，大约 100 多次迭代 PR 值就能收敛到一个稳定的值，而当阻尼系数接近 1 时，需要的迭代次数会陡然增加很多，且排序不稳定。除此之外，PageRank 是实际使用时可能还有大量的改进和优化，这里我们仅了解最基本的思想。

网页之间的链接关系可以用图表示，那么怎么把一个文档或句子（可以看作词的序列）构建成图呢？TextRank 将某一个词与其前面的 N 个词、以及后面的 N 个词均具有图相邻关系（类似于 N-gram 语法模型）。具体实现：设置一个长度为 N 的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点，然后采用共现关系（co-occurrence）构造任两点之间的边。TextRank 构建的词图为无向图。下图给出了由一个文档构建的词图（去掉了停用词并按词性做了筛选）：

[![img](https://lumingdong.cn/wp-content/uploads/2019/02/TextRank-example.png)](https://lumingdong.cn/wp-content/uploads/2019/02/TextRank-example.png)

考虑到不同词对可能有不同的共现（co-occurrence），TextRank 将共现作为无向图边的权值，那些有共现关系的会互相支持对方成为关键词。

TextRank 的迭代计算公式如下：



WS(Vi)=(1−d)+d∗∑j∈In(Vi)wji∑Vk∈Out(Vj)wjkWS(Vj)



可以看出，该公式仅仅比 PageRank 多了一个权重项 wji，用来表示两个节点之间的边连接有不同的重要程度。那么 wji 怎么计算呢？我们举个简单的例子，把 “textrank” 这个单词看做是一个句子，每个字母代表一个经过处理后的词，我们设定窗口为 5，经过滑动，可以构成一下几组：

1. [t, e, x, t, r]
2. [e, x, t, r, a]
3. [x, t, r, a, n]
4. [t, r, a, n, k]

由上可知，（'t'， 'e'）词对共现的次数是 2，即 wji=2；（'e'，'x'）词对共现的次数是 3，即 wji=3，以此类推。

综上，TextRank 用于关键词提取的算法过程如下：

1）把给定的文本 T 按照完整句子进行分割，即

T=[S1,S2,⋯,Sm]

2）对于每个句子 Si 属于 T，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词，即

Si=[ti,1,ti,2,⋯,ti,n]

其中 ti,j 是保留后的候选关键词；

3）构建候选关键词图 G=(V,E)，其中 V 为节点集，由上一步生成的候选关键词组成，然后采用共现关系构造任两点之间的边，两个节点之间存在边仅当它们对应的词汇在长度为 K 的窗口中共现，K 表示窗口大小，即最多共现 K 个单词；

4）根据 TextRank 公式，开始迭代传播各节点的权重，所有词初始化的权重都是 1；

5）每个节点把自己的权重平均分配给 “和自己有连接 “的其他节点；

6）每个节点将所有其他节点分给自己的权重求和，作为自己的新权重；

7）如此反复迭代第 5、6 两步，直到所有的节点权重收敛为止；

8）对节点权重进行倒序排序，从而得到最重要的 T 个单词，作为候选关键词；

9）由上一步得到最重要的 T 个单词，在原始文本中进行标记，若形成相邻词组，则组合成多词关键词，例如，文本中有句子 “Matlab code for plotting ambiguity function”，如果 “Matlab” 和 “code” 均属于候选关键词，则组合成 “Matlab code” 加入关键词序列。

除了可以做关键词提取，TextRank 也可用于提取摘要，即从文本中提取重要的句子，思路与关键词提取基本一致，只是将文本中的每个句子分别看做一个节点，如果两个句子有相似性，那么认为这两个句子对应的节点之间存在一条无向有权边，即权值从原来是词的共现关系变成了句子的相似度。

句子相似度计算公式：



Similarity(Si,Sj)=|{wk|wk∈Si&wk∈Sj}|log(|Si|)+log(|Sj|)



其中，Si、Sj 分别表示两个句子词的集合，Wk 表示句子中的词，分子部分的意思是同时出现在两个句子中的同一个词的个数，分母部分表示对句子中词的个数求对数之和。分母这样设计可以遏制较长的句子在相似度计算上的优势[4](https://lumingdong.cn/text-content-analysis-algorithm.html#dfref-footnote-4)。

TextRank 的文本摘要是一种抽取式的无监督方法，具体流程如下[5](https://lumingdong.cn/text-content-analysis-algorithm.html#dfref-footnote-5)：

[![img](https://lumingdong.cn/wp-content/uploads/2019/02/1545989405612.png)](https://lumingdong.cn/wp-content/uploads/2019/02/1545989405612.png)

1）第一步是把所有文章整合成文本数据

2）接下来把文本分割成单个句子

3）然后，我们将为每个句子找到向量表示（词向量）。

4）根据以上相似度公式循环计算任意两个句子向量间的相似度并存放在矩阵中（实际使用大都会用矩阵存放边权值）；

5）根据阈值去掉两个节点之间相似度较低的边连接，然后将相似矩阵转换为以句子为节点、相似性得分为边的图结构，用于句子 TextRank 计算。

6）最后，一定数量的排名最高的句子构成最后的摘要。

## *3.*词嵌入（Word Embedding）

Embedding，即嵌入，在推荐算法领域非常重要，尤其是和深度学习结合的推荐算法，Embedding 层几乎成了标配，它已经不仅仅局限于内容推荐了。词嵌入表示模型最早应用在自然语言处理领域中，利用背景信息构建词汇的分布式表示。**嵌入式表示模型往往简单而又相对比较有效，因此很快被人们应用到推荐系统中。其核心思想就是同时构建用户和物品的嵌入式表示，使得多种实体的嵌入式表示存在于同一个隐含空间内， 进而可以计算两个实体之间的相似性**。

前面我们介绍了两种关键字提取算法，TF-IDF 是基于 TF-IDF 仅仅从词的统计信息出发，而没有充分考虑词之间的语义信息，TextRank 则考虑了相邻词的语义关系，然而在自然语言中，还有两种非常常见的情况，即一词多义或多词一义的问题，用前面那两种方法都不能很好的解决这两个问题，因此有了词嵌入，它用一个抽象的稠密向量来表征一个词。

嵌入，本身是一个数学概念。我们这里通俗理解，如果将词看作是文本的最小单元，词嵌入本身是一种特别的映射过程，即将文本空间中的某个词，通过一定的方法，**映射**或者说**嵌入（embedding）**到另一个数值向量空间，原来的整数全部变为实数，是用连续向量表示离散变量的方法。之所以称之为 **embedding**，是因为这种表示方法往往伴随着一种降维的意思，就像高维事物拍扁了嵌入到另一个低维空间中一样。

[![img](https://lumingdong.cn/wp-content/uploads/2019/02/6cbb8645gw1f5tns7711ej20j90dkdjx.jpg)](https://lumingdong.cn/wp-content/uploads/2019/02/6cbb8645gw1f5tns7711ej20j90dkdjx.jpg)

因此，我们了解到了词嵌入具有一种降维的特性，这就是词嵌入的另一大优势。前面介绍的结构化方案，可以从中得到一些标签，这些标签都使用 One-Hot 方式的向量化表示，如果整体使用的词典非常大，那么这些向量都是高维稀疏的向量，对于[机器学习](http://lumingdong.cn/category/tech/ml)的参数学习和相关计算都不太友好，而词嵌入则能够为每一个词学习得到一个稠密的向量，维度成倍降低。

词嵌入的维度表示某个隐含语义，一个词可能隐藏很多语义信息，比如北京，可能包含 “首都、中国、北方、直辖市、大城市” 等等，这些语义在所有文本上是有限的，比如 128 个，于是每个词就用一个 128 维的向量表达，向量中各个维度上的值大小代表了词包含各个语义的多少。

现今流行的 **Word Embedding** 算法携带了语义信息且维度经过压缩便于运算，因此有了很多用武之地，例如：

- **计算相似度**，比如 **man** 和 **woman** 的相似度比 **man** 和 **apple** 的相似度高
- **在一组单词中找出与众不同的一个**，例如在如下词汇列表中：**[dog, cat, chicken, boy]**，利用词向量可以识别出 **boy** 和其他三个词不是一类。
- **直接进行词的运算**，例如经典的：**woman+king-man =queen**
- **表征文本**，如累加得到一个文本的稠密向量；
- **方便聚类**，会得到比使用词向量聚类更好的语义聚类效果。
- **由于携带了语义信息，还可以计算一段文字出现的可能性**，也就是说，这段文字是否**通顺**。

本质上来说，经过 **Word Embedding** 之后，各个 **word** 就组合成了一个相对低维空间上的一组向量，这些向量之间的远近关系则由他们之间的语义关系决定。

词嵌入表示通常会用到谷歌提出的 Word2Vec，另外 TensorFlow 中也集成了这部分功能 。Word2Vec 是用浅层神经网络学习得到每个词的向量表达，Word2Vec 最大的贡献在于一些工程技巧上的优化，使得百万词的规模在单机上可以几分钟轻松跑出来，得到这些词向量后可以聚类或者进一步合成句子向量再使用。

Word2Vec 有两种网络结构，分别是 CBOW（Continues Bag of Words）和 Skip-gram，两种网络结构图见下图。

[![img](https://lumingdong.cn/wp-content/uploads/2019/02/word2vec-net-1024x642.jpg)](https://lumingdong.cn/wp-content/uploads/2019/02/word2vec-net.jpg)

其中 w(t) 是当前所关注的词，w(t−2)、w(t−1)、w(t+1)、w(t+2) 是上下文中出现的词。这里前后滑动窗口大小均设为 2。

CBOW 和 Skip-gram 都可以表示成由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络。

输入层中的每个词由独热编码方式表示，即所有词均表示成一个 N 维向量，其中 N 为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为 1，其余维度的值均设为 0。

在映射层（又称隐含层）中，K 个隐含单元（Hidden Units）的取值可以由 N 维输入向量以及连接输入和隐含单元之间的 N×K 维权重矩阵计算得到。在 CBOW 中，还需要将各个输入词所计算出的隐含单元求和。

同理，输出层向量的值可以通过隐含层向量（K 维），以及连接隐含层和输出层之间的 K×N 维权重矩阵计算得到。输出层也是一个 N 维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用 Softmax 激活函数，可以计算出每个单词的生成概率。Softmax 激活函数的定义为：



P(y=wn|x)=exn∑Nk=1exk



其中 x 代表 N 维的原始输出向量，xn 为在原始输出向量中，与单词 wn 所对应维度的取值。

接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为 N×K 的权重矩阵，从隐含层到输出层又需要一个维度为 K×N 的权重矩阵，学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。但是由于 Softmax 激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，由此产生了 Hierarchical Softmax 和 Negative Sampling 两种改进方法，有兴趣可以参考 Word2Vec 的原论文。训练得到维度为 N×K 和 K×N 的两个权重矩阵之后，可以选择其中一个作为 N 个词的 K 维向量表示。

最后我们谈谈 Word2Vec 与 LDA 的区别和联系，首先，LDA 是利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对 “文档-单词” 矩阵进行分解，得到 “文档-主题” 和 “主题-单词” 两个概率分布。而 Word2Vec 其实是对 “上下文-单词” 矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。也就是说，如果两个单词所对应的 Word2Vec 向量相似度较高，那么它们很可能经常在同样的上下文中出现。需要说明的是，上述分析的是 LDA 与 Word2Vec 的不同，不应该作为主题模型和词嵌入两类方法的主要差异。主题模型通过一定的结构调整可以基于 “上下文-单词” 矩阵进行主题推理。同样地，词嵌入方法也可以根据 “文档-单词” 矩阵学习出词的隐含向量表示。主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题）；而词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重以得到单词的稠密向量表示。[6](https://lumingdong.cn/text-content-analysis-algorithm.html#dfref-footnote-6)

## *4.*内容分类

**内容分类可以表达较粗粒度的结构化信息，被很多推荐系统用来在用户冷启动时探索用户兴趣。**在新闻资讯、图文信息流等应用中，经常看到有频道、栏目等大的分类体系，这就是内容分类的应用。内容分类一般由业务专家或产品经理、编辑等人员按照业务内容进行设计的，有些应用在内容发布的时候会强制要求内容发布者指定内容发布到哪个分类下面，这样便可以免去后面再分类的步骤。还有一些应用，可能因为自身业务特性，不便于做显示分类或者无法要求用户做到内容的强制分类发布，此时便需要有个内容分类的步骤去理解内容，看这些内容表达什么类别的信息，以**辅助识别用户的兴趣或匹配用户兴趣进行推荐**。

另外，还可以换一个方向，**即对用户的搜索行为进行理解，通过用户的查询关键字了解用户搜索背后的意图，比如是导航目的、信息目的还是交易目的，对用户的查询关键字做分类是了解用户查询意图的一个简单有效的途径，进而可以获取用户的短期需求。**

内容的分类本质上就是提取文本特征后的多分类问题，因此大部分多类分类器模型都可适用。如传统机器学习算法，常见的有：NB，RF，SVM，KNN，NN 等，对于短文本分类，比较经典的是 SVM 算法。深度学习可用的工具也很多，最常用的是 Facebook 开源的 FastText。

## *5.*实体识别

命名实体识别（Named Entity Recognition，简称 NER），又称作 “专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、著作、机构名、专有名词、历史事件和热点事件等，再具体一点，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体，是信息提取、问答系统、句法分析、机器翻译、面向 Semantic Web 的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位，常用基于词典的方法结合 CRF 模型。

命名实体识别在 [NLP](https://lumingdong.cn/tag/nlp) 技术中常常被认为是序列标注问题，和分词、词性标注属于同一类问题。

所谓序列标注问题，就是给你一个字符序列，从左往右遍历每个字符，一边遍历一边对每一个字符分类，分类的体系因序列标注问题不同而不同：

1. 分词问题：对每一个字符分类为 “词开始”“词中间”“词结束” 三类之一；
2. 词性标注：对每一个分好的词，分类为定义的词性集合的之一；
3. 实体识别：对每一个分好的词，识别为定义的命名实体集合之一。

对于序列标注问题，通常的算法就是隐马尔科夫模型（HMM）或者条件随机场（CRF），我们在推荐系统中主要是挖掘出想要的结构化结果，对其中原理有兴趣再去深入了解。

实体识别还有比较实用化的非模型做法：词典法。提前准备好各种实体的词典，使用 trie-tree 数据结构存储，拿着分好的词去词典里找，找到了某个词就认为是提前定义好的实体了。

以实体识别为代表的序列标注问题上，工业级别的工具上 spaCy 比 NLTK 在效率上优秀一些。

## *6.*聚类

传统聚类方法在文本中的应用，今天逐渐被主题模型取代，同样是无监督模型，以 LDA 为代表的主题模型能够更准确地抓住主题，并且能够得到软聚类的效果，也就是说可以让一条文本属于多个类簇。

产品初期，如果没有业务专家制定分类体系，就可以在文本数据上跑一个 LDA 模型，利用聚类思想出几个主题。

LDA 模型需要设定主题个数，如果你有时间，那么这个 K 可以通过一些实验来对比挑选，方法是：每次计算 K 个主题两两之间的平均相似度，选择一个较低的 K 值；如果你赶时间，在推荐系统领域，只要计算资源够用，主题数可以尽量多一些。

另外，需要注意的是，得到文本在各个主题上的分布，可以保留概率最大的前几个主题作为文本的主题。LDA 工程上较难的是并行化，如果文本数量没到海量程度，提高单机配置也是可以的，开源的 LDA 训练工具有 Gensim，PLDA 等可供选择。