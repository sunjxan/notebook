1. 示例日志user_action.log：
```
{"actionTime":"2019-04-10 18:15:35","readTime":"","channelId":0,"param":{"action": "exposure", "userId": "2", "articleId": "[18577, 14299]", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:12:11","readTime":"2886","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:32","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:34","readTime":"1053","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:36","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "18577", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:38","readTime":"1621","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18577", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:39","readTime":"","channelId":18,"param":{"action": "collect", "userId": "1", "articleId": "14299", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:39","readTime":"","channelId":18,"param":{"action": "share", "userId": "2", "articleId": "14299", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:41","readTime":"914","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "14299", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:47","readTime":"7256","channelId":18,"param":{"action": "read", "userId": "1", "articleId": "14299", "algorithmCombine": "C2"}}
```

2. 创建hive表：

```
create table user_action(
    actionTime STRING,
    readTime STRING,
    channelId INT,
    param MAP<STRING, STRING>
) PARTITIONED BY(dt STRING)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe';
```

3. 启动Kafka
```
cd /usr/local
kafka-server-start.sh kafka/config/server.properties >/dev/null 2>&1 &
```
4. 创建topic
```
kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic user_action
```
5. 配置Flume

在flume/conf目录下创建user_action.conf配置文件

```
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

a1.sources.r1.type = exec
a1.sources.r1.command = tail -F <日志文件路径>
a1.sources.s1.interceptors = i1 i2
a1.sources.s1.interceptors.i1.type = regex_filter
a1.sources.s1.interceptors.i1.regex = \\{.*\\}
a1.sources.s1.interceptors.i2.type = timestamp
 
a1.channels.c1.type = memory
a1.channels.c1.capacity = 30000
a1.channels.c1.transactionCapacity = 1000

a1.channels.c2.type = memory
a1.channels.c2.capacity = 30000
a1.channels.c2.transactionCapacity = 1000

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://<服务器IP>:9000/user/hive/warehouse/<数据库名>.db/user_action/dt=%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollSize = 10240
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.idleTimeout = 60

a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k2.topic = user_action
a1.sinks.k2.brokerList = <服务器IP>:9092
a1.sinks.k2.requiredAcks = 1
a1.sinks.k2.batchSize = 20

a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2
```

6. 启动Flume

```
cd /usr/local
flume-ng agent -c flume/conf -f flume/conf/user_action.conf -n a1 -Dflume.root.logger=INFO,console >/dev/null 2>&1 &
```

7. 查看Hive表

```
alter table user_action add partition (dt='2020-09-03');
select * from  user_action;
```

8. 查看Kafka数据
```
kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic user_action
```

9. 测试新数据
```
echo \{\"actionTime\":\"2019-04-10 21:04:39\",\"readTime\":\"\",\"channelId\":19,\"param\":\{\"action\":\"click\",\"userId\":\"2\",\"articleId\":\"14299\",\"algorithmCombine\":\"C2\"\}\} >> <日志文件路径>
```
10. 创建运行脚本
```
vim user_action_kafka.sh

#!/bin/sh
kafka-server-start.sh kafka/config/server.properties


vim user_action_flume.sh

#!/bin/sh
flume-ng agent -c flume/conf -f flume/conf/user_action.conf -n a1 -Dflume.root.logger=INFO,console
```
11. Supervisor进程管理

```
sudo vim /etc/supervisor/conf.d/user_action_kafka.ini

[program:user_action_kafka]
command=bash <脚本路径>
directory=/usr/local
autostart=false
startsecs=1
autorestart=true
startretries=3
redirect_stderr=false
stdout_logfile=/tmp/user_action_kafka_stdout.log
stderr_logfile=/tmp/user_action_kafka_stderr.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true


sudo vim /etc/supervisor/conf.d/user_action_flume.ini

[program:user_action_flume]
command=bash <脚本路径>
directory=/usr/local
autostart=false
startsecs=1
autorestart=true
startretries=3
redirect_stderr=false
stdout_logfile=/tmp/user_action_flume_stdout.log
stderr_logfile=/tmp/user_action_flume_stderr.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true

# 启动
supervisord
supervisorctl start user_action_kafka
supervisorctl start user_action_flume

# 查看进程状态
supervisorctl status
```

