### 信息量
信息量是用来衡量单一随机变量发生时所含信息的的多寡，随机变量发生的概率越低，其发生后消除系统不确定性的能力越强，所含信息量就越大。

考虑一个**离散的随机变量** x，由上面两个例子可知，信息的量度应该依赖于概率分布 p(x)，因此我们想要寻找一个函数 I(x)，它是概率 p(x) 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 x 和 y，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：I(x,y)=I(x)+I(y)。

因为两个事件是独立不相关的，因此 p(x,y)=p(x)p(y)。根据这两个关系，很容易看出 **I(x)一定与 p(x)的对数有关** 。因此，我们有
$$
I(X)=log_{e}(\frac{1}{P(X)})=-log_{e}P(X)
$$
**其中负号是用来保证信息量是正数或者零。而 log 函数基的选择是任意的**（信息论中基常常选择为2，因此信息的单位为比特bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特nats）。I(x)也被称为随机变量 x 的**自信息 (self-information)，描述的是随机变量的某个事件发生所带来的信息量。图像如图：**

![img](https://images2018.cnblogs.com/blog/1361042/201804/1361042-20180406203059738-480089508.png)


可见该函数符合我们对信息量的直觉。
### 熵(信息熵)
熵(信息熵)可被认为是系统不确定性(混乱程度)的度量，熵值越大，系统越混乱。一个X值域为{x1, ..., xn}的随机变量的熵值H定义为：
$$
H(X)=E(I(X))=-\sum_{i= 1}^{n}p(x_{i})log(p(x_{i}))
$$
**注意点：**1、熵只依赖于随机变量的分布,与随机变量取值无关，所以也可以将 X 的熵记作 H(p)。2、令0log0=0(因为某个取值概率可能为0)。

熵满足如下不等式：
$$
0\leq H(X)\leq log|X|
$$

|X|是X的取值个数，当且仅当X的分布是均匀分布式时右边的等号成立，也就是说X服从均匀分布式时，熵最大。

可以看出，熵是信息量的期望值，是一个随机变量（一个系统，事件所有可能性）不确定性的度量。熵值越大，随机变量的取值就越难确定，系统也就越不稳定；熵值越小，随机变量的取值也就越容易确定，系统越稳定。

### 联合熵

将一维随机变量分布推广到多维随机变量分布，则其**联合熵 (Joint entropy)** 为：
$$
H(X,Y)=−\sum_{x,y}p(x,y)log(p(x,y))=−\sum_{i=1}^{n}\sum_{j=1}^{m}p(x_{i},y_{i})log(p(x_{i},y_{i}))
$$

### 条件熵

条件熵 H(Y|X)表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。条件熵 H(Y|X)定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的数学期望：
$$
H(Y|X)=\sum_{x}p(x)H(Y|X=x)\\
=-\sum_{x}p(x)\sum_{y}p(y|x)log(p(y|x))\\
=-\sum_{x}\sum_{y}p(x,y)log(p(y|x))\\
=-\sum_{x,y}p(x,y)log(p(y|x))
$$
条件熵 H(Y|X)相当于联合熵 H(X,Y) 减去单独的熵 H(X)，即：

$$
H(Y|X)=H(X,Y)−H(X)
$$

举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 H(X,Y)，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 H(X)对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知 H(X) 这个信息量的时候，H(X,Y) 剩下的信息量就是条件熵。因此，可以这样理解，**描述 X 和 Y 所需的信息是描述 X 自己所需的信息,加上给定 X 的条件下具体化 Y 所需的额外信息**。

### 相对熵(KL散度)

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]。直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。

KL散度的计算公式：
$$
D_{KL}(p||q)=\sum_{i=1}^{n}p(x_{i})log(\frac{p(x_{i})}{q(x_{i})})
$$
相对熵具有以下性质：

- 如果p(x)和q(x)的分布相同，则其相对熵等于0

- KL(p∥q)≠KL(q∥p)，也就是相对熵不具有对称性

- KL(p∥q)≥0

  总的来说，相对熵是用来衡量同一个随机变量的两个不同分布之间的距离。**在实际应用中，假如p(x)是目标真实的分布，而q(x)是预测得来的分布，为了让这两个分布尽可能的相同的，就需要最小化KL散度。**

### 交叉熵

交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。

相对熵的公式变形得到：
$$
D_{KL}(p||q)=\sum_{i=1}^{n}p(x_{i})log(p(x_{i}))-\sum_{i=1}^{n}p(x_{i})log(q(x_{i}))\\
=[-\sum_{i=1}^{n}p(x_{i})log(q(x_{i}))]-H(p)
$$

等式的前一部分就是交叉熵：
$$
H(p,q)=-\sum_{i=1}^{n}p(x_{i})log(q(x_{i}))
$$
因为 KL(p||q)≥0，所以 H(p,q)≥H(p)（当 p(x)=q(x)时取等号，此时交叉熵等于信息熵）。

**当 H(p) 为常量时（注：在机器学习中，训练数据分布是固定的），最小化相对熵 KL(p||q) 等价于最小化交叉熵 H(p,q)也等价于最大似然估计。**

根据之前的描述，最小化训练数据上的分布 P(train)与最小化模型分布 P(model)的差异等价于最小化相对熵，即 KL(P(train)||P(model))。此时， P(train)就是KL(p||q 中的 p，即真实分布，P(model)就是 q。又因为训练数据的分布 p 是给定的，所以求 KL(p||q)等价于求 H(p,q)。**得证，交叉熵可以用来计算学习模型分布与训练分布之间的差异**。交叉熵广泛用于逻辑回归的Sigmoid和Softmax函数中作为损失函数使用。

### 最大熵模型

最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。

