### 数据

得到一个有限的训练数据集合，并进行**数据预处理**和**特征工程**。

- 中心化/零均值化
- 归一化  min-max归一化、mean归一化
- 标准化
- 主成分分析
- 白化

### 模型

机器学习首要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个。

- **判别模型**(discriminative model)通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。

  线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（SVM）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）

- **生成模型**（generative model）通过对观测值和标注数据计算联合概率分布P(x,y)来达到判定估算y的目的。

  朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（HMM）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）、混合高斯模型

### 策略

有了模型的假设空间，机器学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。

除了一些预测概率的模型，可以直接以最大化样本出现概率为目标之外，我们引入损失函数的概念。

损失函数（lossfunction）或代价函数（costfunction）度量模型一次预测错误的程度，是f(X)和Y的非负实值函数，记作L(Y, f(X))。

常用的损失函数有以下几种：

1. 0-1损失函数（0-1 loss function）
$$
L(Y, f(X))=\left\{\begin{array}{ll}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right.
$$
2. 平方损失函数（quadratic loss function）
$$
L(Y, f(X))=(Y-f(X))^{2}
$$
3. 绝对损失函数（absolute loss function）
$$
L(Y, f(X))=|Y-f(X)|
$$
4. 对数损失函数（logarithmic loss function）或对数似然损失函数（loglikelihood loss function）
$$
L(Y, P(Y | X))=-\log P(Y | X)
$$
5. 交叉熵损失（cross entropy loss function）
$$
L(Y, f(X))=\sum_{i=1}^{N} -y_{i} \log \left(f\left(x_{i}\right)\right)
$$
损失函数值越小，模型就越好。由于模型的输入、输出（X,Y）是随机变量，遵循联合分布P(X,Y)，所以损失函数的期望是理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失，称为风险函数（risk function）或期望损失（expected loss）。
$$
R_{exp}(f)=E_{p}[L(Y, f(X))]=\int_{x \times y} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{d} y
$$
模型f(X)关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss）。
$$
R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$
期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险。所以一个很自然的想法是用经验风险估计期望风险。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略：经验风险最小化和结构风险最小化。

经验风险最小化（empirical risk minimization，ERM）的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题。
$$
\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
$$
当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。比如，最大似然估计（maximum likelihood estimation）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于最大似然估计。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合(over-fitting)”现象。

结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化（regularization）。结构风险在经验风险上加上表示模型复杂度的正则化项（regularizer）或罚项（penalty term）。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是
$$
R_{\mathrm{srm}}(f)=\frac{1}{N} \sum_{t=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)
$$
其中J(f)为模型的复杂度，是定义在假设空间上的泛函。模型f越复杂，复杂度J(f)就越大；反之，模型f越简单，复杂度J(f)就越小。也就是说，复杂度表示了对复杂模型的惩罚。λ≥0是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小。

结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。比如，贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation，MAP）就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。

结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求最优模型，就是求解最优化问题：
$$
\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)
$$
这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数。

### 算法

算法是指学习模型的具体计算方法。机器学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。这时，机器学习问题归结为最优化问题。

如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的迭代方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。

- 梯度下降法
- 次梯度法
- 坐标轴下降法
- 牛顿法

**机器学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，机器学习的方法也就确定了。这也就是将其称为机器学习三要素的原因。**

### 评估

机器学习的目的是使学到的模型不仅对已知数据而且对未知数据都能有很好的预测能力。不同的学习方法会给出不同的模型。当损失函数给定时，基于损失函数的模型的训练误差（training error）和模型的测试误差（test error）就自然成为学习方法评估的标准。注意，统计学习方法具体采用的损失函数未必是评估时使用的损失函数。当然，让两者一致是比较理想的。

如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。

但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。

- 简单交叉验证
  首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集（例如，70%的数据为训练集，30%的数据为测试集）；然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。
- S折交叉验证（S-fold cross validation）
  首先随机地将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。
- 留一交叉验证
  S折交叉验证的特殊情形是S＝N，称为留一交叉验证（leave-one-out cross validation），往往在数据缺乏的情况下使用。这里，N是给定数据集的容量。
- 自助法（bootstrapping）
以自助采样法（bootstrap samling）为基础，对包含N个样本的数据集，进行N次有放回采样，可以得到约63.2%的样本，用作训练集，其余用作测试集。