概率和统计是一个东西吗？
概率（probabilty）和统计（statistics）看似两个相近的概念，其实研究的问题刚好相反。

概率研究的问题是，已知一个模型和参数，怎么去预测这个模型产生的结果的特性（例如均值，方差，协方差等等）。 举个例子，我想研究怎么养猪（模型是猪），我选好了想养的品种、喂养方式、猪棚的设计等等（选择参数），我想知道我养出来的猪大概能有多肥，肉质怎么样（预测结果）。

统计研究的问题则相反。统计是，有一堆数据，要利用这堆数据去预测模型和参数。仍以猪为例。现在我买到了一堆肉，通过观察和判断，我确定这是猪肉（这就确定了模型。在实际研究中，也是通过观察数据推测模型是／像高斯分布的、指数分布的、拉普拉斯分布的等等），然后，可以进一步研究，判定这猪的品种、这是圈养猪还是跑山猪还是网易猪，等等（推测模型参数）。

一句话总结：概率是已知模型和参数，推数据。统计是已知数据，推模型和参数。

随机变量的数字特征：
- 数学期望
$$
E(X)=\sum_{k=1}^{\infty}x_{k}p_{k}\\
E(X)=\int_{-\infty}^{+\infty}xf(x)dx
$$

- 方差
$$
D(X)=Var(X)=E\{[X-E(X)]^{2}\}=E(X^{2})-[E(X)]^{2}
$$

- 标准差（均方差）
$$
\sigma(X)=\sqrt{D(X)}
$$

概率论的基本理论是极限定理：
- 大数定律
  1. 独立同分布的随机变量，变量的算术平均依概率收敛到数学期望。
  2. 独立重复试验中，事件发生频率依概率收敛到概率。
- 中心极限定理
  1. 独立同分布的随机变量(μ,σ^2 )且方差不为0，当变量数量n充分大时，变量的算术平均近似服从正态分布N(μ,σ^2/n)。
  2. 独立的随机变量且每个变量的方差不为0，当变量数量充分大时，变量的和近似服从正态分布。

样本是进行统计推断的依据，在应用时，往往是根据问题构造样本的函数进行统计推断。从样本中计算出一个不含未知参数的量称为统计量，统计量可以看作一个随机变量，它的值是根据样本值计算出来的观察值。统计量的分布称为抽样分布。

总体X均值为μ，方差为σ^2，样本容量为n：
$$
\frac{1}{n}\sum_{i=1}^{n}{x_{i}}\sim(μ,\frac{σ^{2}}{n})\\
\sqrt{\frac{1}{n}\sum_{i=1}^{n}{x_{i}^{2}}}\sim(μ,σ^{2})
$$


统计推断的基本问题可以分为两大类，一类是估计问题，另一类是假设检验问题。在估计问题中，从样本中计算出一个不含未知参数的量称为统计量，如果用一个统计量的观察值作为某个未知参数的近似值，则称该统计量为估计量，观察值为估计值。

选择估计量的标准：
- 无偏性  估计量的数学期望等于待估参数
- 有效性  估计量的方差越小则越有效
- 相合性  样本容量无限大时估计量依概率收敛于待估参数

无偏估计量

$$
\text{均值} \frac{1}{n}\sum_{i=1}^{n}X_{i}\\
\text{方差} \frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\\
\text{标准差}\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}}
$$